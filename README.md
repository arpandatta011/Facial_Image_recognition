# Facial_Image_recognition üßë‚Äçü¶∞üë©

![](https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2018/10/EmoPy-Computer-Vision/en/resources/1emotions-1540772527761.png)

Human facial expressions can be easily classified into 7 basic emotions: happy, 
sad, surprise, fear, anger, disgust, and neutral. Our facial emotions are 
expressed through activation of specific sets of facial muscles. These 
sometimes subtle, yet complex, signals in an expression often contain an 
abundant amount of information about our state of mind. Through facial 
emotion recognition, we are able to measure the effects that content and 
services have on the audience/users through an easy and low-cost procedure. 
For example, retailers may use these metrics to evaluate customer interest. 
Healthcare providers can provide better service by using additional information 
about patients' emotional state during treatment. Entertainment producers can 
monitor audience engagement in events to consistently create desired content.
Humans are well-trained in reading the emotions of others, in fact, at just 14 
months old, babies can already tell the difference between happy and sad. But 
can computers do a better job than us in accessing emotional states? To answer 
the question, We designed a deep learning neural network that gives machines 
the ability to make inferences about our emotional states. In other words, we 
give them eyes to see what we can see.

<img align = "centre">![image](https://user-images.githubusercontent.com/93440576/169586658-fa16812e-e3c6-4e5a-9d0f-bd60102e790e.png)








