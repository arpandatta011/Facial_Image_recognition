# Facial_Image_recognition üßë‚Äçü¶∞üë©

![](https://imgopt.infoq.com/fit-in/1200x2400/filters:quality(80)/filters:no_upscale()/news/2018/10/EmoPy-Computer-Vision/en/resources/1emotions-1540772527761.png)

Human facial expressions can be easily classified into 7 basic emotions: happy, 
sad, surprise, fear, anger, disgust, and neutral. Our facial emotions are 
expressed through activation of specific sets of facial muscles. These 
sometimes subtle, yet complex, signals in an expression often contain an 
abundant amount of information about our state of mind. Through facial 
emotion recognition, we are able to measure the effects that content and 
services have on the audience/users through an easy and low-cost procedure. 
For example, retailers may use these metrics to evaluate customer interest. 
Healthcare providers can provide better service by using additional information 
about patients' emotional state during treatment. Entertainment producers can 
monitor audience engagement in events to consistently create desired content.
Humans are well-trained in reading the emotions of others, in fact, at just 14 
months old, babies can already tell the difference between happy and sad. But 
can computers do a better job than us in accessing emotional states? To answer 
the question, We designed a deep learning neural network that gives machines 
the ability to make inferences about our emotional states. In other words, we 
give them eyes to see what we can see.


### About the Dataset 

The dataset, used for training the model is from a Kaggle Facial Expression 
ecognition Challenge a few years back (FER2013). The data consists of 48x48 
pixel grayscale images of faces. The faces have been automatically registered 
so that the face is more or less centered and occupies about the same amount of 
space in each image. The task is to categorize each face based on the emotion 
shown in the facial expression in to one of seven categories (0=Angry, 
1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).
The training set consists of 28,709 examples. The public test set used for the 
leaderboard consists of 3,589 examples. The final test set, which was used to 
determine the winner of the competition, consists of another 3,589 examples. 
Emotion labels in the dataset:
0: -4593 images- Angry
1: -547 images- Disgust
2: -5121 images- Fear
3: -8989 images- Happy
4: -6077 images- Sad
5: -4002 images- Surprise
6: -6198 images- Neutral

### CONCLUSION üí°

The facial expression recognition system presented in this research work 
contributes a resilient face recognition model based on the mapping of 
behavioral characteristics with the physiological biometric characteristics. The 
physiological characteristics of the human face with relevance to various 
expressions such as happiness, sadness, fear, anger, surprise and disgust are 
associated with geometrical structures which restored as base matching emplate 
for the recognition system.The behavioral aspect of this system relates the 
attitude behind different expressions as property base. The property bases are 
alienated as exposed and hidden category in genetic algorithmic genes. The 
gene training set evaluates the expressional uniqueness of individual faces and 
provide a resilient expressional recognition model in the field of biometric 
security. The design of a novel asymmetric cryptosystem based on biometrics 
having features like hierarchical group security eliminates the use of passwords 
and smart cards as opposed to earlier cryptosystems. It requires a special
hardware support like all other biometrics system. This research work promises 
a new direction of research in the field of asymmetric biometric cryptosystems 
which is highly desirable in order to get rid of passwords and smart cards 
completely. Experimental analysis and study show that the hierarchical security 
structures are effective in geometric shape identification for physiological
traits



